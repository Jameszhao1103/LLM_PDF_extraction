# Document Extraction System

This project is a document extraction system that utilizes PyMuPDF for extracting content from PDF files and interacts with the DeepSeek API to process the extracted content. The system is specifically designed to extract key information from Chinese legal documents and financial statements, with outputs saved in JSON format and convertible to Excel.

## Project Structure

```
document-extraction-system
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ main.py               # Command-line entry point
â”‚   â”œâ”€â”€ pdf_extractor.py      # PDF extraction functionalities using PyMuPDF
â”‚   â”œâ”€â”€ llm_client.py         # Interaction with the DeepSeek API using OpenAI client
â”‚   â”œâ”€â”€ json_handler.py       # JSON file handling and Excel conversion
â”‚   â”œâ”€â”€ batch_processor.py    # Multi-threaded batch processing engine
â”‚   â””â”€â”€ config.py             # Configuration settings (API keys, prompts)
â”œâ”€â”€ streamlit_app.py          # Streamlit web interface
â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ __init__.py           # Marks the tests directory as a package
â”‚   â”œâ”€â”€ test_pdf_extractor.py # Unit tests for PDFExtractor
â”‚   â”œâ”€â”€ test_llm_client.py    # Unit tests for LLMClient
â”‚   â””â”€â”€ test_json_handler.py  # Unit tests for JSONHandler
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ input                 # Directory for input PDF files
â”‚   â””â”€â”€ output                # Directory for output JSON files and Excel reports
â”œâ”€â”€ requirements.txt          # Project dependencies
â””â”€â”€ README.md                 # Project documentation
```

## Features

- **ğŸŒ Web Interface**: Modern Streamlit-based web interface for easy document processing
- **âš¡ Multi-threaded Processing**: Concurrent processing of multiple PDF files for improved performance
- **ğŸ“„ PDF Text Extraction**: Extract text content from PDF files using PyMuPDF
- **ğŸ¤– AI-Powered Analysis**: Process extracted content using DeepSeek API for structured data extraction
- **ğŸ“Š Real-time Progress Tracking**: Live progress updates during batch processing with success/failure metrics
- **ğŸ§µ Configurable Threading**: Adjustable number of worker threads (1-16) based on system capabilities
- **ğŸ“ˆ Batch Processing**: Process multiple PDF files automatically with detailed reporting
- **ğŸ“‹ Excel Export**: Convert all extracted JSON data to Excel format for easy analysis
- **ğŸ‡¨ğŸ‡³ Chinese Language Support**: Optimized for Chinese legal and financial documents
- **ğŸ“‹ Structured Output**: Extract specific fields like dates, amounts, company names, etc.
- **â±ï¸ Performance Metrics**: Processing time tracking and throughput statistics

## Setup Instructions

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd document-extraction-system
   ```

2. **Install dependencies**:
   It is recommended to use a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scripts\activate`
   pip install -r requirements.txt
   ```

3. **Configure API settings**:
   Edit `src/config.py` and add your DeepSeek API key:
   ```python
   DEEPSEEK_API_KEY = "your_api_key_here"
   ```

4. **Create data directories**:
   ```bash
   mkdir -p data/input data/output
   ```

## Usage

The system provides two interfaces:

### ğŸŒ Web Interface (Recommended)

Launch the Streamlit web application:
```bash
streamlit run streamlit_app.py
```

**Web Interface Features:**
- **ğŸ“ Extraction Prompt Configuration**: Customize the AI extraction prompt
- **ğŸ“„ Single File Processing**: Upload and process individual PDF files
- **ğŸ“š Batch Processing**: 
  - Select multiple files from a directory
  - Configure number of threads (1-16)
  - Real-time progress tracking
  - Live success/failure metrics
- **ğŸ“Š Results & Export**: View processing results and export to Excel

**Multi-threading Configuration:**
- Adjustable worker threads (default: 4)
- Real-time processing metrics
- Estimated completion time
- Thread-safe progress updates

### ğŸ’» Command Line Interface

The system supports multiple CLI operation modes:

#### 1. Process a Single PDF File
```bash
python src/main.py <path_to_pdf_file>
```

#### 2. Test PDF Extraction (Debug Mode)
```bash
python src/main.py test <path_to_pdf_file>
```

#### 3. Multi-threaded Batch Processing
```bash
python src/main.py batch [input_directory]
```
If no input directory is specified, it defaults to `data/input/`

**Batch Processing Features:**
- Concurrent processing using configurable worker threads
- Automatic rate limiting to respect API limits
- Comprehensive error handling and logging
- Detailed processing statistics and summaries

#### 4. Convert JSON Outputs to Excel
```bash
python src/main.py excel
```

#### 5. Preview Extracted Data
```bash
python src/main.py preview
```

## Examples

### Web Interface Examples

1. **Start the web application**:
   ```bash
   streamlit run streamlit_app.py
   ```

2. **Configure threading for batch processing**:
   - Navigate to "Batch Processing" tab
   - Set number of threads (recommended: 4-8 for most systems)
   - Select files to process
   - Monitor real-time progress

### Command Line Examples

1. **Process a single legal document**:
   ```bash
   python src/main.py data/input/court_decision.pdf
   ```

2. **Test extraction without API call**:
   ```bash
   python src/main.py test data/input/sample.pdf
   ```

3. **Multi-threaded batch processing**:
   ```bash
   python src/main.py batch data/input/
   ```

4. **Convert all processed files to Excel**:
   ```bash
   python src/main.py excel
   ```

## Performance Optimization

### Multi-threading Configuration

The system uses configurable multi-threading for optimal performance:

- **Default threads**: 4 workers
- **Recommended range**: 4-8 threads for most systems
- **Maximum**: 16 threads (adjust based on API rate limits)
- **Rate limiting**: Built-in delays to respect API constraints

### Performance Metrics

The system provides detailed performance tracking:
- Processing time per file
- Average throughput (files/second)
- Success/failure rates
- Text extraction statistics
- Memory usage optimization

## Output Format

The system extracts the following information from Chinese legal documents:

- **æ¡ˆä»¶å·** (Case Number)
- **åŸå‘Šå±…ä½åŸå¸‚** (Plaintiff's City of Residence)
- **åŸå‘Šä»£ç†å¾‹å¸ˆæ‰€å±å¾‹æ‰€** (Plaintiff's Law Firm)
- **åŸå‘Šå§”æ‰˜ä»£ç†äººå§“å** (Plaintiff's Attorney Name)
- **è™šå‡é™ˆè¿°å®æ–½æ—¥** (False Statement Implementation Date)
- **åŸå‘Šä¸»å¼ è™šå‡é™ˆè¿°æ­éœ²æ—¥** (Plaintiff's Claimed Disclosure Date)
- **è¢«å‘Šä¸»å¼ è™šå‡é™ˆè¿°æ­éœ²æ—¥** (Defendant's Claimed Disclosure Date)
- **æ³•é™¢è®¤å®šè™šå‡é™ˆè¿°æ­éœ²æ—¥** (Court-Determined Disclosure Date)
- **è™šå‡é™ˆè¿°åŸºå‡†æ—¥** (False Statement Base Date)
- **æ›´æ­£æ—¥** (Correction Date)
- **æŠ•èµ„è€…ä¹°å…¥/å–å‡ºè‚¡ç¥¨æ—¥æœŸ** (Investor Buy/Sell Dates)
- **æ³•é™¢ç«‹æ¡ˆ/è£åˆ¤æ—¥æœŸ** (Court Filing/Judgment Dates)
- **è¯‰è®¼è¯·æ±‚é‡‘é¢** (Litigation Claim Amount)
- **å®é™…æ”¯æŒé‡‘é¢** (Actual Support Amount)
- **è¢«å‘Šåç§°** (Defendant Name)
- **è™šå‡é™ˆè¿°ç§ç±»** (Type of False Statement)
- **æŠ•èµ„è€…æœªè·èµ”å¿ç†ç”±** (Reason for Non-compensation)
- **æ˜¯å¦é‡‡å–ç¬¬ä¸‰æ–¹æ ¸å®šè®¡ç®—ä¹¦** (Third-party Verification Used)
- **æ³•é™¢è®¡ç®—æŠ•èµ„è€…æŸå¤±æ—¶ä½¿ç”¨çš„æ–¹æ³•** (Court's Loss Calculation Method)
- **é‡å¤§æ€§** (Materiality)
- **æ˜¯å¦è¿å¸¦ä¸­ä»‹æœºæ„** (Intermediary Institution Liability)
- **æ˜¯å¦è¿å¸¦é«˜ç®¡æˆ–è‡ªç„¶äºº** (Executive/Individual Liability)
- **çœä»½** (Province)

## Dependencies

### Core Dependencies
- **PyMuPDF**: PDF text extraction
- **OpenAI**: API client for DeepSeek integration
- **pandas**: Data manipulation and Excel export
- **openpyxl**: Excel file format support

### Web Interface Dependencies
- **Streamlit**: Web application framework
- **threading**: Multi-threading support
- **concurrent.futures**: Advanced threading capabilities

### Development Dependencies
- **pytest**: Unit testing framework
- **logging**: Comprehensive logging system

## Configuration

### API Configuration
```python
# src/config.py
DEEPSEEK_API_KEY = "your_api_key_here"
DEEPSEEK_API_URL = "https://api.deepseek.com"
MODEL_NAME = "deepseek-chat"
```

### Performance Configuration
```python
# Threading settings
MAX_WORKERS = 4  # Adjust based on system capabilities
API_RATE_LIMIT = 60  # requests per minute
REQUEST_DELAY = 1.0  # seconds between requests
```

## Workflow

1. **ğŸ”§ Configure**: Set up API credentials and processing parameters
2. **ğŸ“„ Extract**: PyMuPDF extracts text from PDF files
3. **ğŸ§µ Process**: Multi-threaded processing with DeepSeek API analysis
4. **ğŸ“Š Structure**: AI outputs structured JSON data with predefined fields
5. **ğŸ“ˆ Monitor**: Real-time progress tracking and performance metrics
6. **ğŸ“‹ Export**: JSON data converted to Excel for analysis

## Troubleshooting

### Common Issues

1. **API Rate Limits**: Adjust `REQUEST_DELAY` in config.py
2. **Memory Issues**: Reduce `MAX_WORKERS` for large files
3. **Threading Errors**: Check system thread limits

### Performance Tips

1. **Optimal Thread Count**: Start with 4 threads, adjust based on performance
2. **Large Batches**: Process in smaller batches for better memory management
3. **API Efficiency**: Monitor API usage to avoid rate limiting

## Contributing

Contributions are welcome! Please consider:

- Multi-threading optimizations
- UI/UX improvements for the Streamlit interface
- Additional document format support
- Performance enhancements

Please open an issue or submit a pull request for any enhancements or bug fixes.

## License

This project is licensed under the MIT License. See the LICENSE file for more details.